{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60bb447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "676c3912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset loaded successfully!\n",
      "Shape: (1599999, 6)\n",
      "   sentiment          id                          date     query  \\\n",
      "0          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
      "1          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
      "2          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "3          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "4          0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
      "\n",
      "            user                                               text  \n",
      "0  scotthamilton  is upset that he can't update his Facebook by ...  \n",
      "1       mattycus  @Kenichan I dived many times for the ball. Man...  \n",
      "2        ElleCTF    my whole body feels itchy and like its on fire   \n",
      "3         Karoli  @nationwideclass no, it's not behaving at all....  \n",
      "4       joy_wolf                      @Kwesidei not the whole crew   \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path from notebooks folder to data/raw folder\n",
    "csv_path = \"../data/raw/training.1600000.processed.noemoticon.csv\"\n",
    "\n",
    "try:\n",
    "    dataset = pd.read_csv(csv_path, encoding='latin-1')\n",
    "    print(\"✓ Dataset loaded successfully!\")\n",
    "    print(f\"Shape: {dataset.shape}\")\n",
    "    \n",
    "    # Add proper column names\n",
    "    column_names = ['sentiment', 'id', 'date', 'query', 'user', 'text']\n",
    "    dataset.columns = column_names\n",
    "    print(dataset.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "887942bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date     query  \\\n",
       "0          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "1          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "3          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "\n",
       "            user                                               text  \n",
       "0  scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "1       mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "2        ElleCTF    my whole body feels itchy and like its on fire   \n",
       "3         Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "4       joy_wolf                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e10b214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599999, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69141a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    0\n",
       "id           0\n",
       "date         0\n",
       "query        0\n",
       "user         0\n",
       "text         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "794a324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5ae95ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599999, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bef61e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.replace({'sentiment': {4:1}},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9be9abfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    800000\n",
       "0    799999\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff7581c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vs Preprocessed text (first 5 rows):\n",
      "                                                text  \\\n",
      "0  is upset that he can't update his Facebook by ...   \n",
      "1  @Kenichan I dived many times for the ball. Man...   \n",
      "2    my whole body feels itchy and like its on fire    \n",
      "3  @nationwideclass no, it's not behaving at all....   \n",
      "4                      @Kwesidei not the whole crew    \n",
      "\n",
      "                                   preprocessed_text  \n",
      "0  upset update facebook texting might cry result...  \n",
      "1  kenichan dived many time ball managed save res...  \n",
      "2                    whole body feel itchy like fire  \n",
      "3                   nationwideclass behaving mad see  \n",
      "4                                kwesidei whole crew  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK data (run this once)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocesses text by:\n",
    "    1. Converting to lowercase\n",
    "    2. Removing non-alphabetic characters\n",
    "    3. Tokenizing into words\n",
    "    4. Removing stopwords\n",
    "    5. Lemmatizing words\n",
    "    6. Joining back into a string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle missing/null values\n",
    "        if pd.isna(text) or text is None:\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to string if not already\n",
    "        text = str(text)\n",
    "        \n",
    "        # Initialize lemmatizer and get stopwords\n",
    "        word_net = WordNetLemmatizer()\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove non-alphabetic characters (keep only letters and spaces)\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        \n",
    "        # Split into words\n",
    "        words = text.split()\n",
    "        \n",
    "        # Remove stopwords and lemmatize\n",
    "        processed_words = [\n",
    "            word_net.lemmatize(word) \n",
    "            for word in words \n",
    "            if word not in stop_words and len(word) > 2  # Also remove very short words\n",
    "        ]\n",
    "        \n",
    "        # Join back into string\n",
    "        processed_text = ' '.join(processed_words)\n",
    "        \n",
    "        return processed_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text[:50]}... Error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Now apply the preprocessing\n",
    "dataset['preprocessed_text'] = dataset['text'].apply(preprocess)\n",
    "\n",
    "# Check the results\n",
    "print(\"Original vs Preprocessed text (first 5 rows):\")\n",
    "print(dataset[['text', 'preprocessed_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "068b7d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessed data saved!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(\"notebooks/preprocess\", exist_ok=True)\n",
    "\n",
    "# Now save the file\n",
    "dataset.to_csv(\"notebooks/preprocess/imdb_clean.csv\", index=False)\n",
    "print(\"✅ Preprocessed data saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
